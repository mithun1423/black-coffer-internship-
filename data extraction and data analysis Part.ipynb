{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wdZNdP--aQ2"
      },
      "source": [
        "# New section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5nOC4rqVq9Y",
        "outputId": "e1742ecc-f279-423f-8433-f371def57342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "\n",
        "# Install necessary libraries if not already installed\n",
        "!pip install pandas requests beautifulsoup4\n",
        "\n",
        "# Replace 'Input.xlsx' with the actual path to your file if needed\n",
        "input_file = 'Input.xlsx'\n",
        "\n",
        "try:\n",
        "    # Load the Excel file into a pandas DataFrame\n",
        "    df = pd.read_excel(input_file)\n",
        "\n",
        "    # Create a directory to store the extracted text files\n",
        "    output_dir = 'extracted_articles'\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Iterate through each row (article) in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        url_id = row['URL_ID']\n",
        "        url = row['URL']\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # ---  Content Extraction Logic (Adapt this part for different websites) ---\n",
        "            # Example 1: Find the main article content using a common class or tag\n",
        "            article_content = soup.find('article') # or soup.find('div', class_='article-body')\n",
        "\n",
        "            # Example 2: If the above doesn't work, try a more specific approach (Inspect element to find article body)\n",
        "            if article_content is None:\n",
        "              article_content = soup.find('div', {'id': 'article-body'}) # Or any relevant identifier\n",
        "\n",
        "            #Fallback if both of the above failed\n",
        "            if article_content is None:\n",
        "              article_content = soup.find('div', class_='main-content')\n",
        "\n",
        "            if article_content:\n",
        "              #Extract article title\n",
        "              article_title = soup.find('h1')\n",
        "              title_text = article_title.text.strip() if article_title else \"No Title Found\"\n",
        "\n",
        "              # Extract text from the article\n",
        "              text = ''\n",
        "              for paragraph in article_content.find_all(['p', 'h2']):  # Include other tags as needed\n",
        "                  text += paragraph.get_text(strip=True) + '\\n'\n",
        "\n",
        "              # Save the extracted text to a file\n",
        "              with open(os.path.join(output_dir, f'{url_id}.txt'), 'w', encoding='utf-8') as f:\n",
        "                  f.write(title_text + '\\n\\n' + text)\n",
        "\n",
        "            else:\n",
        "                print(f\"Could not extract article content for URL_ID: {url_id}, URL:{url}\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching URL {url}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred for URL_ID: {url_id}, URL:{url}: {e}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: Input file '{input_file}' not found.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMA61ofQdQxX",
        "outputId": "9b418a82-6ece-488a-db63-205db5eea79c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-43-33d6b6315b84>:112: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  output_df = pd.concat([output_df, pd.DataFrame([new_row])], ignore_index=True)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "\n",
        "# Download required NLTK data (only needed once)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "try:\n",
        "    word_tokenize(\"test\")\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "\n",
        "def analyze_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords and punctuation\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [w.lower() for w in tokens if w.isalnum() and w.lower() not in stop_words]\n",
        "\n",
        "    # Calculate variables\n",
        "    positive_score = 0\n",
        "    negative_score = 0\n",
        "    polarity_score = 0\n",
        "    subjectivity_score = 0\n",
        "    avg_sentence_length = 0\n",
        "    percentage_complex_words = 0\n",
        "    fog_index = 0\n",
        "    avg_number_of_words_per_sentence = 0\n",
        "    complex_word_count = 0\n",
        "    word_count = len(filtered_tokens)\n",
        "    syllable_count_per_word = 0\n",
        "    personal_pronouns = 0\n",
        "    avg_word_length = 0\n",
        "\n",
        "    if word_count > 0:\n",
        "      # TextBlob analysis\n",
        "      analysis = TextBlob(text)\n",
        "      polarity_score = analysis.sentiment.polarity\n",
        "      subjectivity_score = analysis.sentiment.subjectivity\n",
        "\n",
        "      # Sentence Length and Complex Word calculations (simplified examples)\n",
        "      sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text) # Split into sentences\n",
        "      avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences) if len(sentences) > 0 else 0\n",
        "\n",
        "      #Counting Complex words (words with more than 2 syllables)\n",
        "      for word in filtered_tokens:\n",
        "        vowels = \"aeiouy\"\n",
        "        syllable_count = 0\n",
        "        for i in range(len(word)):\n",
        "          if word[i] in vowels:\n",
        "            syllable_count += 1\n",
        "        if syllable_count > 2:\n",
        "          complex_word_count += 1\n",
        "\n",
        "      percentage_complex_words = (complex_word_count/ word_count) * 100 if word_count > 0 else 0\n",
        "\n",
        "      fog_index = 0.4*(avg_sentence_length + percentage_complex_words) # Simplified fog index calculation\n",
        "\n",
        "      avg_number_of_words_per_sentence = avg_sentence_length\n",
        "\n",
        "      #Syllable Count per word (approximation)\n",
        "      syllable_count_per_word = sum(len(re.findall(r'[aeiouy]+', word, re.IGNORECASE)) for word in filtered_tokens) / len(filtered_tokens) if len(filtered_tokens) > 0 else 0\n",
        "\n",
        "      #Counting personal pronouns\n",
        "      for word in tokens:\n",
        "          if word.lower() in [\"i\", \"we\", \"my\", \"ours\", \"us\"]:\n",
        "              personal_pronouns+=1\n",
        "\n",
        "      #Average word length\n",
        "      avg_word_length = sum(len(word) for word in filtered_tokens) / len(filtered_tokens) if len(filtered_tokens) > 0 else 0\n",
        "\n",
        "      #Adjusting polarity and subjectivity score to fit into the desired scale\n",
        "      positive_score = max(0, polarity_score)\n",
        "      negative_score = max(0, -polarity_score)\n",
        "\n",
        "\n",
        "    return [positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "            avg_sentence_length, percentage_complex_words, fog_index,\n",
        "            avg_number_of_words_per_sentence, complex_word_count, word_count,\n",
        "            syllable_count_per_word, personal_pronouns, avg_word_length]\n",
        "\n",
        "\n",
        "# Output Data Structure\n",
        "output_df = pd.DataFrame(columns=['URL_ID', 'positive_score', 'negative_score', 'polarity_score', 'subjectivity_score',\n",
        "                                  'avg_sentence_length', 'percentage_complex_words', 'fog_index',\n",
        "                                  'avg_number_of_words_per_sentence', 'complex_word_count', 'word_count',\n",
        "                                  'syllable_per_word', 'personal_pronouns', 'avg_word_length'])\n",
        "\n",
        "output_directory = 'extracted_articles'  # Directory with the extracted text files\n",
        "for filename in os.listdir(output_directory):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        url_id = filename[:-4]  # Remove the .txt extension\n",
        "\n",
        "        with open(os.path.join(output_directory, filename), 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        results = analyze_text(text)\n",
        "        new_row = {'URL_ID': url_id, 'positive_score': results[0], 'negative_score':results[1], 'polarity_score':results[2], 'subjectivity_score':results[3],\n",
        "                  'avg_sentence_length':results[4], 'percentage_complex_words':results[5], 'fog_index':results[6],\n",
        "                  'avg_number_of_words_per_sentence':results[7], 'complex_word_count':results[8], 'word_count':results[9],\n",
        "                  'syllable_per_word':results[10], 'personal_pronouns':results[11], 'avg_word_length':results[12]}\n",
        "\n",
        "        output_df = pd.concat([output_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "# Save the results to an Excel file\n",
        "output_df.to_excel('Output_Data.xlsx', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "31pr5AeydnYn"
      },
      "outputs": [],
      "source": [
        "# prompt: Definition of each of the variables given in the “Text Analysis.docx” file.\n",
        "# Look for these variables in the analysis document (Text Analysis.docx):\n",
        "# 1.\tPOSITIVE SCORE\n",
        "# 2.\tNEGATIVE SCORE\n",
        "# 3.\tPOLARITY SCORE\n",
        "# 4.\tSUBJECTIVITY SCORE\n",
        "# 5.\tAVG SENTENCE LENGTH\n",
        "# 6.\tPERCENTAGE OF COMPLEX WORDS\n",
        "# 7.\tFOG INDEX\n",
        "# 8.\tAVG NUMBER OF WORDS PER SENTENCE\n",
        "# 9.\tCOMPLEX WORD COUNT\n",
        "# 10.\tWORD COUNT\n",
        "# 11.\tSYLLABLE PER WORD\n",
        "# 12.\tPERSONAL PRONOUNS\n",
        "# 13.\tAVG WORD LENGTH\n",
        "# /content/Text Analysis.docx\n",
        "\n",
        "# Definition of variables as per the provided Python code:\n",
        "\n",
        "# 1. POSITIVE SCORE:  The positive score is derived from the polarity score.  It represents the positive sentiment expressed in the text.\n",
        "#    It's calculated as the maximum of 0 and the polarity score.  A higher positive score indicates more positive sentiment.  It's adjusted from the polarity score of TextBlob to fit a scale where negative score cannot be negative.\n",
        "\n",
        "# 2. NEGATIVE SCORE: The negative score is also derived from the polarity score. It represents the negative sentiment in the text.\n",
        "#    It's calculated as the maximum of 0 and the absolute value of the negative polarity score.  A higher negative score indicates more negative sentiment.  It's adjusted from the polarity score of TextBlob to fit a scale where negative score cannot be negative.\n",
        "\n",
        "# 3. POLARITY SCORE:  This represents the overall sentiment expressed in the text.  It is calculated using TextBlob's sentiment analysis.\n",
        "#    The polarity score is a float within the range [-1.0, 1.0], where -1.0 represents extremely negative sentiment, 1.0 represents extremely positive sentiment, and 0.0 represents neutral sentiment.\n",
        "\n",
        "# 4. SUBJECTIVITY SCORE: This score represents the subjectivity of the text, i.e., how much personal opinion or judgment is present.\n",
        "#    It's calculated using TextBlob's sentiment analysis.  The subjectivity score is a float within the range [0.0, 1.0], where 0.0 is very objective and 1.0 is very subjective.\n",
        "\n",
        "# 5. AVG SENTENCE LENGTH: The average number of words per sentence in the text. It's calculated by summing the number of words in each sentence and dividing by the total number of sentences.\n",
        "\n",
        "# 6. PERCENTAGE OF COMPLEX WORDS: The percentage of words in the text that are considered complex.  In this code, a complex word is defined as a word with more than two syllables. The code uses a vowel counting heuristic to approximate syllable count.\n",
        "\n",
        "# 7. FOG INDEX:  A readability score calculated using the formula 0.4 * (average sentence length + percentage of complex words). It estimates the years of formal education needed to understand the text.  This code uses a simplified formula.\n",
        "\n",
        "# 8. AVG NUMBER OF WORDS PER SENTENCE:  This is the same as the average sentence length (point 5).\n",
        "\n",
        "# 9. COMPLEX WORD COUNT: The total number of words in the text that are classified as complex (more than two syllables, approximated by vowel count).\n",
        "\n",
        "# 10. WORD COUNT: The total number of words in the text after removing stop words and punctuation.\n",
        "\n",
        "# 11. SYLLABLE PER WORD: The average number of syllables per word in the text.  The syllable count is approximated using a vowel counting heuristic.\n",
        "\n",
        "# 12. PERSONAL PRONOUNS: The number of personal pronouns (e.g., \"I,\" \"we,\" \"my,\" \"ours,\" \"us\") found in the text.\n",
        "\n",
        "# 13. AVG WORD LENGTH: The average length (in characters) of the words in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRuaruOlhNmQ",
        "outputId": "e6c9cb68-7b5e-4d7b-dabb-b811efaf892d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: syllapy in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install pandas nltk textblob syllapy openpyxl\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcgdM-hbhQI4",
        "outputId": "babbd8ae-f52f-4ede-ac92-a0628edba885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-11-27 07:31:35--  https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/positive-words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20630 (20K) [text/plain]\n",
            "Saving to: ‘positive-words.txt.1’\n",
            "\n",
            "\rpositive-words.txt.   0%[                    ]       0  --.-KB/s               \rpositive-words.txt. 100%[===================>]  20.15K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2024-11-27 07:31:35 (34.8 MB/s) - ‘positive-words.txt.1’ saved [20630/20630]\n",
            "\n",
            "--2024-11-27 07:31:35--  https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/negative-words.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 46299 (45K) [application/octet-stream]\n",
            "Saving to: ‘negative-words.txt.1’\n",
            "\n",
            "negative-words.txt. 100%[===================>]  45.21K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2024-11-27 07:31:35 (3.93 MB/s) - ‘negative-words.txt.1’ saved [46299/46299]\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/positive-words.txt\n",
        "!wget https://raw.githubusercontent.com/jeffreybreen/twitter-sentiment-analysis-tutorial-201107/master/data/opinion-lexicon-English/negative-words.txt\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from syllapy import count as syllable_count\n",
        "\n",
        "# Load NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load positive and negative word lists, specifying encoding\n",
        "with open('positive-words.txt', 'r', encoding='latin-1') as f:  # Change encoding if needed\n",
        "    positive_words = set(f.read().split())\n",
        "with open('negative-words.txt', 'r', encoding='latin-1') as f:  # Change encoding if needed\n",
        "    negative_words = set(f.read().split())\n",
        "\n",
        "# Load input variables from Input.xlsx\n",
        "input_path = \"Input.xlsx\"\n",
        "input_df = pd.read_excel(input_path)\n",
        "\n",
        "def analyze_text(text):\n",
        "    # Tokenize sentences and words\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    words = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Metrics\n",
        "    positive_score = sum(1 for word in filtered_words if word in positive_words)\n",
        "    negative_score = sum(1 for word in filtered_words if word in negative_words)\n",
        "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(filtered_words) + 0.000001)\n",
        "    avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences) if sentences else 0\n",
        "    word_count = len(filtered_words)\n",
        "    syllable_count_total = sum(syllable_count(word) for word in filtered_words)\n",
        "    syllable_per_word = syllable_count_total / word_count if word_count else 0\n",
        "    complex_words = [word for word in filtered_words if syllable_count(word) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    avg_word_length = sum(len(word) for word in filtered_words) / word_count if word_count else 0\n",
        "    personal_pronouns = sum(1 for word in words if word.lower() in [\"i\", \"we\", \"my\", \"ours\", \"us\"])\n",
        "\n",
        "    # Return metrics\n",
        "    return [positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "            avg_sentence_length, percentage_complex_words, fog_index, complex_word_count, word_count, syllable_per_word, personal_pronouns, avg_word]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMSW74xTj-VP",
        "outputId": "03b4a856-7194-4a6a-eb9d-ed33b6000722"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "<ipython-input-67-52a478497f23>:85: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  output_df = pd.concat([output_df, pd.DataFrame([new_row])], ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Analysis complete. Results saved to Output_Data.xlsx\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "from syllapy import count as syllable_count\n",
        "\n",
        "# Load NLTK data\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load positive and negative word lists, specifying encoding\n",
        "with open('positive-words.txt', 'r', encoding='latin-1') as f:  # Change encoding if needed\n",
        "    positive_words = set(f.read().split())\n",
        "with open('negative-words.txt', 'r', encoding='latin-1') as f:  # Change encoding if needed\n",
        "    negative_words = set(f.read().split())\n",
        "# Load input variables from Input.xlsx\n",
        "input_path = \"Input.xlsx\"\n",
        "input_df = pd.read_excel(input_path)\n",
        "\n",
        "def analyze_text(text):\n",
        "    # Tokenize sentences and words\n",
        "    sentences = sent_tokenize(text)\n",
        "    tokens = word_tokenize(text)\n",
        "    words = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Metrics\n",
        "    positive_score = sum(1 for word in filtered_words if word in positive_words)\n",
        "    negative_score = sum(1 for word in filtered_words if word in negative_words)\n",
        "    polarity_score = (positive_score - negative_score) / (positive_score + negative_score + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(filtered_words) + 0.000001)\n",
        "    avg_sentence_length = sum(len(word_tokenize(sentence)) for sentence in sentences) / len(sentences) if sentences else 0\n",
        "    word_count = len(filtered_words)\n",
        "    syllable_count_total = sum(syllable_count(word) for word in filtered_words)\n",
        "    syllable_per_word = syllable_count_total / word_count if word_count else 0\n",
        "    complex_words = [word for word in filtered_words if syllable_count(word) > 2]\n",
        "    complex_word_count = len(complex_words)\n",
        "    percentage_complex_words = (complex_word_count / word_count) * 100 if word_count else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    avg_word_length = sum(len(word) for word in filtered_words) / word_count if word_count else 0\n",
        "    personal_pronouns = sum(1 for word in words if word.lower() in [\"i\", \"we\", \"my\", \"ours\", \"us\"])\n",
        "\n",
        "    # Return metrics\n",
        "    return [positive_score, negative_score, polarity_score, subjectivity_score,\n",
        "            avg_sentence_length, percentage_complex_words, fog_index,\n",
        "            avg_sentence_length, complex_word_count, word_count,\n",
        "            syllable_per_word, personal_pronouns, avg_word_length]\n",
        "\n",
        "# Initialize output DataFrame with columns matching the desired format\n",
        "output_columns = ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
        "                  'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
        "                  'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
        "                  'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
        "\n",
        "output_df = pd.DataFrame(columns=output_columns)\n",
        "\n",
        "# Directory containing extracted articles\n",
        "output_directory = 'extracted_articles'\n",
        "\n",
        "# Process each file\n",
        "for _, row in input_df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "    file_path = os.path.join(output_directory, f\"{url_id}.txt\")\n",
        "\n",
        "    if os.path.exists(file_path):\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "        results = analyze_text(text)\n",
        "        new_row = {\n",
        "            'URL_ID': url_id, 'URL': url, 'POSITIVE SCORE': results[0], 'NEGATIVE SCORE': results[1],\n",
        "            'POLARITY SCORE': results[2], 'SUBJECTIVITY SCORE': results[3],\n",
        "            'AVG SENTENCE LENGTH': results[4], 'PERCENTAGE OF COMPLEX WORDS': results[5],\n",
        "            'FOG INDEX': results[6], 'AVG NUMBER OF WORDS PER SENTENCE': results[7],\n",
        "            'COMPLEX WORD COUNT': results[8], 'WORD COUNT': results[9],\n",
        "            'SYLLABLE PER WORD': results[10], 'PERSONAL PRONOUNS': results[11],\n",
        "            'AVG WORD LENGTH': results[12]\n",
        "        }\n",
        "        output_df = pd.concat([output_df, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "# Save the output to Excel\n",
        "output_file = \"Output_Data.xlsx\"\n",
        "output_df.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Analysis complete. Results saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "weYKyVZuk-zR"
      },
      "outputs": [],
      "source": [
        "# prompt: convert output_data .xlsx into csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a pandas DataFrame\n",
        "excel_file = 'Output_Data.xlsx'\n",
        "df = pd.read_excel(excel_file)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "csv_file = 'output_data.csv'\n",
        "df.to_csv(csv_file, index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
